# -*- coding: utf-8 -*-
"""Dataverse Hack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16yumy-oJKzg_OcHc1F5wyK9-XU2pwEGj

# ***`Insurance Claim Prediction`***




Predict whether the policyholder will file a claim in the next 6 months or not.

# *** Problem Statement***

* CarIns is a startup that provides insurance for cars. It is one of the best car insurance brands known for the highest claim settlement ratio. It was launched back in Oct 2020 and acquired its initial policyholders by providing a hassle-free claim process, instant policy issuance, and claim settlements at minimum coverages.



* As it's a fast growing startup, the company would like to optimize the cost of the insurance by identifying the policyholders who are more likely to claim in the next 6 months. 


* Now the company would like to use Data Science to identify the policyholders whose chances of filing a claim are high in the next 6 months. The company challenges the Data Science community to build a high-performance algorithm to predict if the policyholder will file a claim in the next 6 months or not based on the set of car and policy features.
"""

# Commented out IPython magic to ensure Python compatibility.
#import Neccessory libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split, GridSearchCV

#import required accuracy metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.model_selection import KFold, cross_val_score

import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

#loading the data set
df = pd.read_csv(r"/content/train.csv")
df.head(5)

#lets check the shape 
print('Shape of train dataset:',df.shape)

#check the data types
df.dtypes

#lets check for Null Values
df.isnull().sum()

df.drop(columns = 'policy_id', inplace = True)

df.head(2)

#Lets check which columns contains '?'
df[df.columns[(df == '?').any()]].nunique()

#Lets chcek the value counts for categorical data
for i in df.columns:
    if df[i].dtypes == 'int64':
        print(df[i].value_counts())
        print('---------'*10)

#Lets chcek the value counts for categorical data
for i in df.columns:
    if df[i].dtypes == 'object':
        print(df[i].value_counts())
        print('---------'*10)

for col in df.columns:
    print(col,df[col].nunique())
    print('-'*35)

"""# **EDA**"""

#lets check distribution for continuous columns
num_data = df._get_numeric_data()
plt.figure(figsize = (20,20))
plotnumber = 1
for column in num_data:
    if plotnumber <=45:
        ax = plt.subplot(12,3,plotnumber)
        sns.distplot(num_data[column])
        plt.xlabel(column,fontsize = 20)
    plotnumber+=1
plt.tight_layout()

"""# **Data processing**

# **Apply label encoder to target variable**
"""

from sklearn.preprocessing import LabelEncoder
leb_enc = LabelEncoder()
df2 = leb_enc.fit_transform(df["is_claim"])
pd.Series(df2)
df["is_claim"] = df2

df.columns

"""# ***Heat map for checking correlation***"""

#Lets plot heatmap to check correlation among differnt features and label
df_corr = df.corr()
plt.figure(figsize = (20,15))
sns.heatmap(df_corr,vmin=-1,vmax=1,annot=True,center=0,fmt='.2g',linewidths=0.1)
plt.tight_layout()

#lets describe the data
df.describe().T

"""## **Checking for outliers using box plots**"""

#lets check outliers from continuous columns
num_data = df._get_numeric_data()
plt.figure(figsize = (25,10))
plotnumber = 1
for column in num_data:
    if plotnumber <=45:
        ax = plt.subplot(5,4,plotnumber)
        sns.boxplot(num_data[column])
        plt.xlabel(column,fontsize = 20)
    plotnumber+=1
plt.tight_layout()

#lets see the destribution of numerical data
num_data = df._get_numeric_data()
plt.figure(figsize = (25,20))
plt.style.use('fivethirtyeight')
plotnumber = 1
for column in num_data:
    if plotnumber <=5:
        ax = plt.subplot(3,2,plotnumber)
        sns.distplot(num_data[column])
        plt.xlabel(column,fontsize = 20)
    plotnumber+=1
plt.tight_layout()

#lets check for skewness
df.skew()

"""# ***Separate features and label as x & y respectively***"""

x = df.drop(columns = 'is_claim')
y = df['is_claim']

df.columns

x.skew()

#Lets treat the skewness from numerical columns
for index in x.skew().index:
    if x.skew().loc[index]>0.5:
        x[index]=np.log1p(x[index])
    if x.skew().loc[index]<-0.5:
        x[index]=np.cbrt(x[index])

#check the skewness again
x.skew()

#lets separate numerical and categorical features for scaling and encoding
num = x._get_numeric_data()
cat = x.select_dtypes(include=['object'])

"""## ***Applying StandardScaler to numerical features***"""

#Lets bring all numerical features to common scale by applying standard scaler
scaler = StandardScaler()
x_num = scaler.fit_transform(num)
x_num = pd.DataFrame(x_num,columns=num.columns)

#combine both numerical and categorical features
X = pd.concat([x_num,cat], axis = 1)

#lets have a look at our features
X.head()

"""# **Encoding**"""

#lets convert categorical data into numeric values, using OrdinalEncoder
from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for i in X.columns:
    if X[i].dtypes == "object" :
        X[i] = enc.fit_transform(X[i].values.reshape(-1,1))

#lets have a look at data after encoding
X.head()

#check the shape
X.shape

#check value count for target variable
y.value_counts()

"""## ***Over sampling***"""

#lets do oversampling using SMOTE
import imblearn
from imblearn.over_sampling import SMOTE
SM = SMOTE()
x_over,y_over = SM.fit_resample(X,y)

#lets check the count of target variable now
y_over.value_counts()

"""# ***Finding Best random state***"""

#Lets find the best random state using LogisticRegression
from sklearn.linear_model import LogisticRegression
max_accu = 0
max_rs = 0
for i in range(50,100):
    x_train,x_test,y_train,y_test = train_test_split(x_over,y_over,test_size = 0.25, random_state = i)
    LR = LogisticRegression()
    LR.fit(x_train,y_train)
    pred = LR.predict(x_test)
    acc = accuracy_score(y_test,pred)
    if acc > max_accu:
        max_accu = acc
        max_rs = i
print("Best accuracy is",max_accu,"on Random State",max_rs)

#lets split our data into train and test parts with best random_state
x_train,x_test,y_train,y_test = train_test_split(x_over, y_over, test_size = 0.25, random_state = 59)

"""## ***Model Building with Evaluation Metrics***

## ***LogisticRegression model***
"""

#Lets check the model with LogisticRegression
LR.fit(x_train,y_train)
predlr = LR.predict(x_test)
accuracy = accuracy_score(y_test,predlr)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,predlr)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,predlr)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,predlr)}")

#cross validation score
scores = cross_val_score(LR, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\nAccuracy Score - Cross Validation Score :", result)

"""# **DecisionTreeClassifier model**"""

#model with DecesionTreeClassifier
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(x_train,y_train)
pred_dt = dt.predict(x_test)
accuracy = accuracy_score(y_test,pred_dt)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,pred_dt)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_dt)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_dt)}")

#cross validation score
scores = cross_val_score(dt, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\n\nAccuracy Score - Cross Validation Score :", result)

"""# **RandomForestClassifier model**"""

#model with RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(x_train,y_train)
pred_rf = model.predict(x_test)
accuracy = accuracy_score(y_test,pred_rf)*100

print(f"Accuracy Score:", accuracy)
print(f"\nroc_auc_score: {roc_auc_score(y_test,pred_rf)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_rf)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_rf)}")

#cross validation score
scores = cross_val_score(rf, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\n\nAccuracy Score - Cross Validation Score :", result)

"""# ***KNeighborsClassifier model***"""

#model with KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(x_train,y_train)
pred_kn = kn.predict(x_test)
accuracy = accuracy_score(y_test,pred_kn)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,pred_kn)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_kn)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_kn)}")

#cross validation score
scores = cross_val_score(kn, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\n\nAccuracy Score - Cross Validation Score :", result)

"""## ***XGBClassifier model***"""

#lets check with XGBClassifier model
from xgboost import XGBClassifier
xgb = XGBClassifier(verbosity = 0)
xgb.fit(x_train,y_train)
pred_xgb = xgb.predict(x_test)
accuracy = accuracy_score(y_test,pred_xgb)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,pred_xgb)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_xgb)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_xgb)}")

#cross validation score
scores = cross_val_score(xgb, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\nAccuracy Score - Cross Validation Score :", result)

"""# **ExtraTreesClassifier model**"""

#lets check with Extra Trees Classifier
from sklearn.ensemble import ExtraTreesClassifier
ext = ExtraTreesClassifier()
ext.fit(x_train,y_train)
pred_ext = xgb.predict(x_test)
accuracy = accuracy_score(y_test,pred_ext)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,pred_ext)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_ext)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_ext)}")

#cross validation score
scores = cross_val_score(ext, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\nAccuracy Score - Cross Validation Score :", result)

"""# ***AUC & ROC Curve***"""

#Lets plot roc curve and check auc and performance of all algorithms
from sklearn.metrics import plot_roc_curve
disp = plot_roc_curve(LR, x_test, y_test)
plot_roc_curve(dt, x_test, y_test, ax = disp.ax_)
plot_roc_curve(rf, x_test, y_test, ax = disp.ax_)
plot_roc_curve(kn, x_test, y_test, ax = disp.ax_)
plot_roc_curve(xgb, x_test, y_test, ax = disp.ax_)
plot_roc_curve(ext, x_test, y_test, ax = disp.ax_)
plt.figure(figsize = (25,25))
plt.show()

"""We can see KNeighborsClassifier is giving least difference in accuracy and cv score but its AUC is very less

RandomForestClassifier is giving least in accuracy and cv score next to KNeighborsClassifier, and its AUC also High, that is it is showing better model performance than KNeighborsClassifier.

ExtraTreesClassifier and XGBClassifier are showing almost same AUC as RandomForestClassifier, but their the difference in accuracy and cv score is higher than RandomForestClassifier.

Considering above observations I am selecting RandomForestClassifier as a best suitable algorithm for this model.

# ***Hyperparameter Tuning***
"""

#lets selects different parameters for tuning
grid_params = {
               'criterion':['gini','entropy'],
                'max_depth': [10,12,15,20,22],
                'n_estimators':[500,700,1000,1200],
                'max_features':['aoto','sqrt','log2'],
                'min_samples_split': [2]
                }

#train the model with given parameters using GridSearchCV
#GCV =  GridSearchCV(RandomForestClassifier(), grid_params, cv = 5)
#GCV.fit(x_train,y_train)

#GCV.best_params_       #printing the best parameters found by GridSearchCV

#lets check the results of final model with best parameters
#model = RandomForestClassifier(criterion = 'gini', max_depth = 22, min_samples_split = 2,  n_estimators = 1200)
#model.fit(x_train,y_train)
#pred = model.predict(x_test)

#print(f"Accuracy Score: {accuracy_score(y_test,pred)*100}%")
#print("--------------------------------------------------------")

#print(f"roc_auc_score: {roc_auc_score(y_test,pred)*100}%")
#print("--------------------------------------------------------")

#print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred)}\n")
#print("------------------------------------------------------------------------")
#print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred)}")

"""# **AUC ROC CURVE for final model**

"""

plot_roc_curve(model, x_test, y_test)
plt.title('ROC Curve for best model')
plt.show()

"""Great after hyperparameter tuning we got improvement in roc curve and AUC also.

# ***Test_Data_Set***
"""

test = pd.read_csv('/content/test.csv')
test.head(5)

L_ID = test['policy_id']
test = test.drop(columns='policy_id')

test.columns

test.shape

test.info()

test.isna().sum()

#Lets check which columns contains '?'
test[test.columns[(test == '?').any()]].nunique()

#lets separate numerical and categorical features for scaling and encoding
num = test._get_numeric_data()
cat = test.select_dtypes(include=['object'])

#Lets bring all numerical features to common scale by applying standard scaler
scaler = StandardScaler()
test_num = scaler.fit_transform(num)
test_num = pd.DataFrame(test_num,columns=num.columns)

#combine both numerical and categorical features
test = pd.concat([test_num,cat], axis = 1)

test.head(5)

#lets convert categorical data into numeric values, using OrdinalEncoder
from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for i in test.columns:
    if test[i].dtypes == "object" :
        test[i] = enc.fit_transform(test[i].values.reshape(-1,1))

test.shape

test.head(5)

#lets predict the price with our best model
prediction = model.predict(test)

prediction

df.columns

#lets make the dataframe for prediction
Loan_st = pd.DataFrame(prediction, columns=["is_claim"])

Loan_st.shape

L_ID.shape

loan_file = pd.concat([L_ID, Loan_st], axis = 1)

loan_file.head(5)

loan_file.shape

#Lets save the submission to csv
loan_file.to_csv("predicted_is_claim.csv",index=False)

z = pd.read_csv('/content/sample_submission.csv')
z.head(5)

"""# ***Thank You***"""